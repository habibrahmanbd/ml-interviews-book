### 7.1 Basics

1. [E] Explain supervised, unsupervised, weakly supervised, semi-supervised, and active learning.

    **Answer:**
    Supervised learning is a type of machine learning where the model is trained on labeled data, meaning that the correct output or label for a given input is provided during the training process. The model then uses this information to make predictions on new, unseen data.

    Unsupervised learning is a type of machine learning where the model is not provided with labeled data during training. Instead, the model must discover the underlying structure or relationships in the data on its own. Common unsupervised learning techniques include clustering and dimensionality reduction.

    Weakly supervised learning is a type of machine learning where the model is provided with weak or incomplete labels during training. This is often used when obtaining fully labeled data is difficult or expensive.

    Semi-supervised learning is a type of machine learning that falls between supervised and unsupervised learning. It's used when a small portion of data is labeled and rest is unlabeled. The model is trained on both labeled and unlabeled data to make predictions.

    Active learning is a type of machine learning where the model is able to actively request labels for specific data points, in order to improve its performance on future predictions. This is often used in situations where obtaining labeled data is difficult or expensive, and can help to improve the overall performance of the model.

2. Empirical risk minimization.
   i. [E] What’s the risk in empirical risk minimization?
   ii. [E] Why is it empirical?
   iii. [E] How do we minimize that risk?

      **Answer:**
      i. The risk in empirical risk minimization refers to the difference between the predicted output of a model and the actual output, also known as the prediction error. This risk is typically measured by a loss function, such as mean squared error.

      ii. Empirical risk minimization is called as such because it is based on the empirical data, rather than theoretical assumptions. The model is trained on a set of training data and the risk is calculated based on the performance of the model on that specific data set.

      iii. To minimize the risk in empirical risk minimization, we use optimization algorithms such as gradient descent to adjust the parameters of the model to minimize the loss function. This process is repeated until the model reaches a satisfactory level of performance on the training data. Additionally, techniques like cross validation can also be used to evaluate the performance of the model on unseen data and further fine-tune the model to minimize risk.


3. [E] Occam's razor states that when the simple explanation and complex explanation both work equally well, the simple explanation is usually correct.  How do we apply this principle in ML?

   **Answer:**
   In machine learning, Occam's razor can be applied in a few ways:

      - Model selection: When choosing a model, it is often easier to understand and interpret a simpler model, such as linear regression, than a more complex model, such as a deep neural network. Therefore, if a simple model can achieve similar performance to a complex model, the simpler model should be chosen.

      - Feature selection: In the process of feature engineering, Occam's razor can be used to select a smaller number of more informative features, instead of using a large number of less informative features.

      - Regularization: Regularization techniques, such as L1 and L2 regularization, can be used to prevent overfitting by adding a penalty term to the loss function that discourages the model from having too many parameters. This helps to keep the model simple and prevent overfitting.

      - Data preprocessing: Simple data preprocessing techniques, such as scaling, normalizing, and removing outliers, can be applied to the data to help improve the performance of a model without the need for complex feature engineering or model selection.

   By applying Occam's razor, we can avoid overfitting and make the model more interpretable, and easy to implement.

4. [E] What are the conditions that allowed deep learning to gain popularity in the last decade?

   **Answer:**
   There are several conditions that allowed deep learning to gain popularity in the last decade:

      - Advancements in computing power: The increased availability of powerful GPUs and the development of distributed computing frameworks, such as TensorFlow and PyTorch, have made it possible to train large and complex deep learning models.

      - Availability of large amounts of data: With the growth of the internet, there is now a vast amount of data available for training deep learning models, including images, text, and audio. This has allowed deep learning models to achieve breakthrough performance in a variety of applications.

      - Development of new architectures: The development of new architectures such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) has allowed deep learning models to be applied to a wide range of problems, from image recognition to natural language processing.

      - Improved optimization algorithms: The development of optimization algorithms such as Adam and RMSprop have made it possible to train deep learning models faster and more effectively.

      - Lower costs: The costs of computing and storage have dropped significantly in recent years, making it more accessible for researchers, startups, and large companies to invest in deep learning.

   These conditions have enabled deep learning to quickly become a popular and powerful tool for solving a wide range of problems, from image and speech recognition to natural language processing and self-driving cars.


5. [M] If we have a wide NN and a deep NN with the same number of parameters, which one is more expressive and why?

   **Answer:**
   If we have a wide neural network (NN) and a deep neural network with the same number of parameters, the deep NN is more expressive. The reason for this is that the deep NN has more layers, which allows for more complex and abstract representations of the data. Each layer in a deep NN can learn different features and representations of the data, and these representations can be combined and transformed in subsequent layers to create a more complex and abstract representation of the data.

   On the other hand, a wide NN has fewer layers but more neurons in each layer. While a wide NN can learn more complex functions with a large number of neurons, it is not able to learn as many abstract representations of the data as a deep NN with the same number of parameters.

   In other words, the number of layers enables the deep NN to learn more abstract features and representations of the data, therefore it is more expressive.


6. [H] The Universal Approximation Theorem states that a neural network with 1 hidden layer can approximate any continuous function for inputs within a specific range. Then why can’t a simple neural network reach an arbitrarily small positive error?

   **Answer:**
   The Universal Approximation Theorem states that a neural network with one hidden layer can approximate any continuous function for inputs within a specific range, but it does not guarantee that the approximation error will be arbitrarily small.

   There are a few reasons why a simple neural network may not reach an arbitrarily small positive error:

      - Limited number of neurons: Even though a neural network with one hidden layer can approximate any continuous function, the number of neurons in the hidden layer may be limited. This limits the capacity of the network and affects its ability to approximate the function with a small error.

      - Overfitting: Neural networks are prone to overfitting, which occurs when the network becomes too complex and memorizes the training data instead of generalizing to new, unseen data. Overfitting can cause the network to have a high error on the test data.

      - Inadequate training data: The Universal Approximation Theorem assumes that the neural network has access to an infinite amount of training data. In practice, the amount of training data is often limited, and the network may not have enough information to approximate the function with a small error.

      - Non-linearity: The function being approximated may be non-linear, and the neural network may not have the capacity to approximate non-linear functions as well as linear ones.

      - Hyperparameters tuning: The neural network's performance depends on the choice of the hyperparameters, such as learning rate, number of neurons, and number of layers. If these hyperparameters are not set correctly, the network may not reach an arbitrarily small positive error.

   In summary, the Universal Approximation Theorem states that a neural network with one hidden layer can approximate any continuous function, but it does not guarantee that the approximation error will be arbitrarily small. There are various factors that can affect the network's ability to approximate the function with a small error, including the number of neurons, overfitting, the amount of training data, and the choice of hyperparameters.

7. [E] What are saddle points and local minima? Which are thought to cause more problems for training large NNs?

   **Answer:**
   A saddle point is a point in a multidimensional space where the gradient is zero in some directions but not in others. In other words, a saddle point is a point where the surface of the function is relatively flat in some dimensions and steep in others.

   A local minimum is a point where the value of the function is lower than its immediate surroundings. In other words, a local minimum is a point where the surface of the function is relatively flat in all dimensions and lower than any nearby points.

   Saddle points are thought to cause more problems for training large neural networks because they can trap optimization algorithms and prevent them from finding the global minimum. Saddle points can be difficult to detect, and so optimization algorithms may spend a lot of time oscillating around them without making progress.


8. Hyperparameters.
    i. [E] What are the differences between parameters and hyperparameters?
    ii. [E] Why is hyperparameter tuning important?
    iii. [M] Explain algorithm for tuning hyperparameters.

      **Answer:**

      i. Parameters are values within a machine learning model that are learned from data during training, while hyperparameters are values that are set before training and control the overall behavior of the model.

      ii. Hyperparameter tuning is important because different hyperparameter settings can significantly affect the performance of a model. By tuning the hyperparameters, we can optimize the performance of a model on a given task.

      iii. There are several algorithms for tuning hyperparameters, including:
      
      - Grid search: This algorithm involves specifying a set of possible values for each hyperparameter, and then training the model with every combination of hyperparameter values. The performance of the model is evaluated for each combination, and the combination that performs the best is selected.
      - Random search: This algorithm involves randomly sampling hyperparameter values from a predefined distribution for each hyperparameter, and then training and evaluating the model. This process is repeated multiple times, and the best combination of hyperparameter values is selected.
      - Bayesian optimization: This algorithm uses a probabilistic model to predict the performance of the model as a function of the hyperparameters. The algorithm then uses this model to select the next set of hyperparameters to evaluate, with the goal of finding the combination that maximizes performance.
      - Genetic Algorithm: This algorithm uses the concept of natural selection and genetic to select the best combination of hyperparameters.
      - Gradient-based optimization: This algorithm optimizes the hyperparameters by computing the gradients of the performance of the model with respect to the hyperparameters and then using optimization algorithms like gradient descent to update them.


9. Classification vs. regression.
    i. [E] What makes a classification problem different from a regression problem?
    ii. [E] Can a classification problem be turned into a regression problem and vice versa?

      **Answer:**

      i. A classification problem is a type of supervised learning problem in which the goal is to predict a categorical label or class for a given input, while a regression problem is a type of supervised learning problem in which the goal is to predict a continuous numeric value for a given input.

      ii. Yes, a classification problem can be turned into a regression problem by predicting a numeric value that represents the class rather than the class label directly. For example, a binary classification problem of identifying whether an image contains a cat or a dog can be turned into a regression problem by predicting a numeric value between 0 and 1, where 0 represents the absence of a cat and 1 represents the presence of a cat. Similarly, a regression problem can be turned into a classification problem by defining a set of threshold values to convert the predicted numeric value into a class label.

10. Parametric vs. non-parametric methods.
    i. [E] What’s the difference between parametric methods and non-parametric methods? Give an example of each method.
    ii. [H] When should we use one and when should we use the other?

      **Answer:**
      
      i. Parametric methods are a class of statistical models that make a specific assumption about the probability distribution of the data. They assume that the data follows a certain probability distribution, such as the normal distribution. Based on this assumption, they estimate a set of parameters that define the distribution, such as the mean and standard deviation. These parameters are used to make predictions about new data points. An example of a parametric method is linear regression, which assumes a normal distribution of errors.

      Non-parametric methods do not make any assumptions about the probability distribution of the data. Instead, they rely on the data itself to estimate the model. They are more flexible and can adapt to any kind of data distribution. An example of non-parametric method is k-nearest neighbors, which makes predictions based on the majority class of the k-closest data points.


      ii. Parametric methods are generally easier to implement and interpret, and they make fewer assumptions about the data. However, if the assumptions made by the parametric method are not met, the predictions may be inaccurate. Non-parametric methods are more flexible and can adapt to any kind of data distribution, but they can be more computationally intensive and harder to interpret.

      In general, it's a good idea to start with a parametric method, and if the assumptions of the method are not met or if the performance is poor, then try non-parametric methods. In cases where the data is very complex or where there is little prior knowledge about the data, non-parametric methods are often preferred.

11. [M] Why does ensembling independently trained models generally improve performance?

   **Answer**

   Ensembling independently trained models is a method of combining multiple models in order to improve performance. There are several reasons why this technique can lead to better performance:
   
   - Diversity: By training multiple models independently, each model is likely to make different predictions for a given input. This diversity in predictions is beneficial, as it allows the ensemble to capture different aspects of the data.

   - Reducing Variance: Some models have high variance, which means that they perform well on the training data but poorly on the test data. Ensembling multiple models can help to reduce the variance of the predictions, by averaging or majority voting the predictions of multiple models.

   - Reducing Bias: Some models have high bias, which means that they don't fit the data well. Ensembling multiple models can help to reduce the bias of the predictions, by combining the predictions of multiple models that have different biases.

   - Handling Model Uncertainty: Ensemble models can also help to handle the uncertainty of predictions. By averaging or majority voting predictions of multiple models, it's possible to get an estimate of the model's uncertainty.

   In summary, ensembling independently trained models can improve performance by capturing different aspects of the data, reducing variance and bias, and handling model uncertainty.


12. [M] Why does L1 regularization tend to lead to sparsity while L2 regularization pushes weights closer to 0?

      **Answer:**
      L1 and L2 regularization are two common methods used to prevent overfitting in machine learning models. L1 regularization, also known as Lasso regularization, tends to lead to sparsity while L2 regularization, also known as Ridge regularization, pushes weights closer to 0.

      L1 regularization adds a penalty term to the cost function that is proportional to the absolute value of the weights. This encourages the model to assign small absolute values to many of the weights, effectively setting them to zero. The resulting model will have many zero-valued weights, leading to a sparse model.
      
      $ J(w) = \frac{1}{2m} \sum_{i=1}^{m}(h_w(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n}|w_j|$

      Here, $J(w)$ is the cost function, $w$ is the vector of model weights, $h_w(x)$ is the hypothesis function, $x^{(i)}$ and $y^{(i)}$ are the input and output values for the i-th training example, $m$ is the number of training examples, $n$ is the number of features, and $\lambda$ is the regularization parameter. The second term, $\lambda \sum_{j=1}^{n}|w_j|$ is the L1 regularization term.

      L2 regularization adds a penalty term to the cost function that is proportional to the square of the weights. This encourages the model to assign small values to the weights, but unlike L1 regularization, it does not encourage the model to set any weights to zero. Instead, it pushes all the weights closer to zero, but none of them would be exactly zero.
      
      $ J(w) = \frac{1}{2m} \sum_{i=1}^{m}(h_w(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n}w_j^2 $

      Here, $J(w)$ is the cost function, $w$ is the vector of model weights, $h_w(x)$ is the hypothesis function, $x^{(i)}$ and $y^{(i)}$ are the input and output values for the i-th training example, $m$ is the number of training examples, $n$ is the number of features, and $\lambda$ is the regularization parameter. The second term, $\frac{\lambda}{2} \sum_{j=1}^{n}w_j^2$ is the L2 regularization term.

      In summary, L1 regularization tends to lead to sparsity because it encourages the model to set many weights to zero, while L2 regularization pushes weights closer to zero by adding a penalty term to the cost function that is proportional to the square of the weights.

      ```python
      from sklearn.linear_model import Lasso, Ridge
      from sklearn.datasets import make_regression

      # Generate synthetic data
      X, y = make_regression(n_samples=1000, n_features=100, random_state=0)

      # Create a Lasso model with a regularization parameter of 0.1
      lasso = Lasso(alpha=0.1)
      lasso.fit(X, y)

      # Create a Ridge model with a regularization parameter of 0.1
      ridge = Ridge(alpha=0.1)
      ridge.fit(X, y)

      # Print the weights of the Lasso and Ridge models
      print("Lasso weights:", lasso.coef_)
      print("Ridge weights:", ridge.coef_)

      ```

13. [E] Why does an ML model’s performance degrade in production?

      **Answer:**
      An ML model's performance can degrade in production for several reasons, including:

      - Overfitting: The model may have been overfitted to the training data, leading to poor performance on new, unseen data.

      - Data shift: The distribution of the data in production may be different from the distribution of the data used to train the model, leading to poor performance.

      - Model complexity: The model may be too complex, leading to poor performance and increased computational resources.

      - Lack of fine-tuning: The model may not have been fine-tuned for the specific data and use case in production, leading to poor performance.

      - Data quality: The quality of the data in production may be different from the quality of the data used to train the model, leading to poor performance.

      - Limited data: The model may have been trained on a limited amount of data, leading to poor performance when exposed to more data.

      - Bias in the data: The data used to train the model may be biased, leading to poor performance when applied to new, unbiased data.

      - Limited resources: The model may be running on a limited computational resources, leading to poor performance.

      - Non-stationary data: The data in production may be changing, leading to poor performance over time.

      It is important to monitor the performance of the model in production and to regularly retrain and fine-tune the model to account for changes in the data and use case.

14. [M] What problems might we run into when deploying large machine learning models?

   **Answer:**
   When deploying large machine learning models, there are several potential issues that may arise, including:

   - Memory constraints: Large models may require significant memory to store and process, which may be a challenge on resource-constrained devices or in cloud environments with limited memory.

   - Computational constraints: Training and deploying large models can be computationally expensive, which may be a challenge on resource-constrained devices or in cloud environments with limited computational resources.

   - Latency: Large models may have high inference latency, which can be a problem for real-time applications or for users with limited bandwidth.

   - Deployment complexity: Large models may be complex to deploy, and may require specialized expertise and infrastructure to deploy effectively.

   - Versioning and rollback: When deploying large models, it can be challenging to rollback to a previous version if something goes wrong.

   - Monitoring and maintenance: Large models require regular monitoring and maintenance to ensure they continue to perform well and to identify and fix any issues that arise.

   - Explainability: Large models may be difficult to interpret and understand, making it challenging to explain their predictions or to identify and fix issues that arise.

   - Security: Large models may be vulnerable to attacks such as model stealing, causing a security concern for the organization.

   - Cost: Large models may be costly to maintain, store, and run.

   To mitigate these issues, it is important to use techniques such as model compression, quantization, and distillation to reduce the size and complexity of the model, and to use techniques such as federated learning, edge computing, and cloud-based deployment to make it easier to deploy and run large models in a variety of environments.

15. Your model performs really well on the test set but poorly in production.
    i. [M] What are your hypotheses about the causes?
    ii. [H] How do you validate whether your hypotheses are correct?
    iii. [M] Imagine your hypotheses about the causes are correct. What would you do to address them?

      **Answer:**

      i. Hypotheses about causes:

      - The model may be overfitting to the test set and not generalizing well to the new data in production.
      - The distribution of the data in production may be different from the distribution of the data used to train and test the model.
      - The model may not have been fine-tuned for the specific data and use case in production.
      - The quality of the data in production may be different from the quality of the data used to train the model.
      - There may be bias in the data used to train the model, leading to poor performance when applied to new, unbiased data.
      - Limited resources may be causing poor performance.
      The data in production may be changing, leading to poor performance over time.

      ii. Validation:

      - Compare the distribution of the data in production to the distribution of the data used to train and test the model.
      - Compare the quality of the data in production to the quality of the data used to train the model.
      - Re-evaluate the model's performance on a held-out test set that is representative of the production data.
      - Monitor the model's performance in production over time.
      - Use techniques such as SHAP values to understand the model's behavior and interpretability.
      - Monitor the resources usage to understand if there is any limitation on the resources.

      iii. Addressing the issues:

      - If the model is overfitting, use techniques such as regularization and dropout to reduce overfitting.
      - If the data distribution in production is different from the data distribution used to train and test the model, retrain the model on a representative sample of the production data.
      - If the model has not been fine-tuned for the specific data and use case in production, fine-tune the model for the specific data and use case.
      - If the quality of the data in production is different from the quality of the data used to train the model, gather more high-quality data to improve the model's performance.
      - If there is bias in the data used to train the model, use techniques such as re-sampling or adversarial training to reduce bias.
      - If limited resources are causing poor performance, consider using techniques such as model compression, quantization, and distillation to reduce the size and complexity of the model.
      - If the data in production is changing, use techniques such as online learning to adapt the model to the changing data.
      - It is important to note that it is often a combination of factors that leads to poor performance in production, and it may be necessary to address multiple issues simultaneously to improve performance.

