### 7.1 Basics

1. [E] Explain supervised, unsupervised, weakly supervised, semi-supervised, and active learning.

    **Answer:**
    Supervised learning is a type of machine learning where the model is trained on labeled data, meaning that the correct output or label for a given input is provided during the training process. The model then uses this information to make predictions on new, unseen data.

    Unsupervised learning is a type of machine learning where the model is not provided with labeled data during training. Instead, the model must discover the underlying structure or relationships in the data on its own. Common unsupervised learning techniques include clustering and dimensionality reduction.

    Weakly supervised learning is a type of machine learning where the model is provided with weak or incomplete labels during training. This is often used when obtaining fully labeled data is difficult or expensive.

    Semi-supervised learning is a type of machine learning that falls between supervised and unsupervised learning. It's used when a small portion of data is labeled and rest is unlabeled. The model is trained on both labeled and unlabeled data to make predictions.

    Active learning is a type of machine learning where the model is able to actively request labels for specific data points, in order to improve its performance on future predictions. This is often used in situations where obtaining labeled data is difficult or expensive, and can help to improve the overall performance of the model.

3. Empirical risk minimization.
    1. [E] What’s the risk in empirical risk minimization?
    2. [E] Why is it empirical?
    3. [E] How do we minimize that risk?
4. [E] Occam's razor states that when the simple explanation and complex explanation both work equally well, the simple explanation is usually correct.  How do we apply this principle in ML?
5. [E] What are the conditions that allowed deep learning to gain popularity in the last decade?
6. [M] If we have a wide NN and a deep NN with the same number of parameters, which one is more expressive and why?
7. [H] The Universal Approximation Theorem states that a neural network with 1 hidden layer can approximate any continuous function for inputs within a specific range. Then why can’t a simple neural network reach an arbitrarily small positive error?
8. [E] What are saddle points and local minima? Which are thought to cause more problems for training large NNs?
9. Hyperparameters.
    4. [E] What are the differences between parameters and hyperparameters?
    5. [E] Why is hyperparameter tuning important?
    6. [M] Explain algorithm for tuning hyperparameters.
10. Classification vs. regression.
    7. [E] What makes a classification problem different from a regression problem?
    8. [E] Can a classification problem be turned into a regression problem and vice versa?
11. Parametric vs. non-parametric methods.
    9. [E] What’s the difference between parametric methods and non-parametric methods? Give an example of each method.
    10. [H] When should we use one and when should we use the other?
12. [M] Why does ensembling independently trained models generally improve performance?
13. [M] Why does L1 regularization tend to lead to sparsity while L2 regularization pushes weights closer to 0?
14. [E] Why does an ML model’s performance degrade in production?
15. [M] What problems might we run into when deploying large machine learning models?
16. Your model performs really well on the test set but poorly in production.
    11. [M] What are your hypotheses about the causes?
    12. [H] How do you validate whether your hypotheses are correct?
    13. [M] Imagine your hypotheses about the causes are correct. What would you do to address them?
